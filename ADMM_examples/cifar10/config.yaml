#config example:
#Authors: Jiahao, Yan
#sample_argument:
#type || default value
#value
#print_freq  int || 10  
#resume      bool || False     *resume from checkpoint
#gpu_id      int || null      * gpu id to use
#lr          float || 0.1      *learning rate
#arch        str  || resnet18  *architecture
#save_model  str  || saved_model.pt * filename to saved model
#load_model  str  || load_model.pt  * filename to load model
#workers     int  || 16        * number of threads for loading images
#logging     bool || False     * whether to log files
#log_dir     str  || logs      * destination folder to drop log file
#smooth_eps  float|| 0.0       * smoothing rate [0.0, 1.0], set to 0.0 to disable
#alpha       float|| 0.0       * for mixup training, lambda = Beta(alpha, alpha) distribution. Set to 0.0 to disable
#                              * chosen value of alpha in Bag of Tricks is 0.2, and increase # of training epochs by 67%
#epochs      int  || 200       * number of training epochs for chosen stage
#warmup_epochs      int  || 0       * number of epochs for lr warmup
#warmup_lr      float  || 0.0001       * initial lr before warmup
#optimizer   str  || sgd       * optimizer for the chosen stage
#lr_scheduler str || default,cosine  * learning rate scheduler
#admm_epoch  int  || 20        * how many epochs required to update z and u
#rho        float || 0.001     * regularization strength
#sparsity_type str || [irregular,column,filter,bn_filter]     * sparsity type
#multi_rho   bool  || False    * whether to use multi rho in admm stage
#verbose    bool  || False     * whether to print convergence info
#masked_progressive bool || False * whether to use masked progressive

adv:
  epsilon:
    8.0    # will be divided by 255 in the adv_train.py
  num_steps:
    10
  step_size:
    2.0   # will be divided by 255 in the adv_train.py
  random_start:
    True
  loss_func:
    xent
  width_multiplier:
    16
  init_func:
    default
general:
 sparsity_type: 
  filter 
 print_freq:
  100 
 resume:  
  False 
 gpu_id:  
  1
 arch: 
  vgg16_adv
 workers: 
  16
 logging: 
  False
 log_dir: 
  logs
 smooth_eps:
  0.0
 alpha:
  0.0
pretrain:
 lr:
  0.1
 epochs: 
  200
 warmup_lr:
  0.0001
 warmup_epochs: 
  0
 save_model: 
  xxx.pt
 optimizer:
  sgd
 lr_scheduler: 
  cosine
admm:
 lr:  
  0.01
 epochs: 
  200
 save_model: 
  xxx_admm.pt
 load_model: 
  xxx_pretrained.pt 
 optimizer:
  sgd
 lr_scheduler: 
  default
 admm_epoch: 
  20
 rho: 
  0.001
 multi_rho: 
  True
 masked_progressive: 
  False
 verbose:
  False
retrain:
 lr:  
  0.01
 epochs:
  200
 warmup_lr:
  0.0001
 warmup_epochs: 
  0
 save_model: 
  xxx_retrained.pt
 load_model:
  xxx_admm.pt
 optimizer: 
  sgd
 lr_scheduler:
  default
 masked_progressive: 
  False 
resnet18:
#20 conv layers, use bnx instead of convx for bn_filter pruning
#floats
 prune_ratios:
  conv1.weight:
   0.9375
  conv2.weight:
   0.9375
  conv3.weight:
   0.9375
  conv4.weight:
   0.9375
  conv5.weight:
   0.9375
  conv6.weight:
   0.9375
  conv7.weight:
   0.9375
  conv8.weight:
   0.9375
  conv9.weight:
   0.9375
  conv10.weight:
   0.9375
  conv11.weight:
   0.9375
  conv12.weight:
   0.9375
  conv13.weight:
   0.9375
  conv14.weight:
   0.9375
  conv15.weight:
   0.9375
  conv16.weight:
   0.9375
  conv17.weight:
   0.9375
  conv18.weight:
   0.9375
  conv19.weight:
   0.9375
  conv20.weight:
   0.9375
vgg16:
#20 conv layers, use bnx instead of convx for bn_filter pruning
#floats
 prune_ratios:
  conv1.weight:
   0.9375
  conv2.weight:
   0.9375
  conv3.weight:
   0.9375
  conv4.weight:
   0.9375
  conv5.weight:
   0.9375
  conv6.weight:
   0.9375
  conv7.weight:
   0.9375
  conv8.weight:
   0.9375
  conv9.weight:
   0.9375
  conv10.weight:
   0.9375
  conv11.weight:
   0.9375
  conv12.weight:
   0.9375
  conv13.weight:
   0.9375
vgg16_adv:
 prune_ratios:
  conv1.weight:
   0.9375
  conv2.weight:
   0.9375
  conv3.weight:
   0.9375
  conv4.weight:
   0.9375
  conv5.weight:
   0.9375
  conv6.weight:
   0.9375
  conv7.weight:
   0.9375
  conv8.weight:
   0.9375
  conv9.weight:
   0.9375
  conv10.weight:
   0.9375
  conv11.weight:
   0.9375
  conv12.weight:
   0.9375
  conv13.weight:
   0.9375
